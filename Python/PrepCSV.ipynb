{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMma+KcQPdtT3B0NaJ0E5gY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"eZ8kC0t3mxzH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677703855025,"user_tz":-180,"elapsed":40352,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"49923bab-f24e-449b-8d37-a4e3c8b71cd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import zipfile\n","path_to_zip = '/content/drive/MyDrive/Samal_work/archive.zip'\n","tar = zipfile.ZipFile(path_to_zip)\n","tar.extractall()\n","tar.close()"],"metadata":{"id":"DPNaBxwIri9K","executionInfo":{"status":"ok","timestamp":1677703965271,"user_tz":-180,"elapsed":110253,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os \n","import glob \n","\n","from PIL import Image, ImageDraw\n","%matplotlib inline \n","from matplotlib import pyplot as plt # this lets you draw inline pictures in the notebooks\n","import pylab # this allows you to control figure size \n","pylab.rcParams['figure.figsize'] = (10.0, 8.0) # this controls figure size in the notebook\n","import cv2\n","import numpy as np\n","from collections import Counter \n","from pathlib import Path\n","import pandas as pd\n","import torch\n","import tqdm\n","from matplotlib.patches import Rectangle\n","import pickle\n","\n","from math import cos, sin, sqrt"],"metadata":{"id":"oMgYnzwq2kgx","executionInfo":{"status":"ok","timestamp":1677703971238,"user_tz":-180,"elapsed":5987,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":[" # SynergyNet importion"],"metadata":{"id":"Fz9yWr3N91ao"}},{"cell_type":"code","source":["print('\\n> Cloning the repo')\n","!git clone https://github.com/choyingw/SynergyNet\n","\n","%cd SynergyNet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aCVKO2l98X3","executionInfo":{"status":"ok","timestamp":1677620755588,"user_tz":-180,"elapsed":31,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"9526df99-505a-4c6c-e124-ea685194b3f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Cloning the repo\n","fatal: destination path 'SynergyNet' already exists and is not an empty directory.\n","/content/SynergyNet/SynergyNet\n"]}]},{"cell_type":"code","source":["%cd Sim3DR\n","!sh ./build_sim3dr.sh\n","\n","%cd ../FaceBoxes\n","!sh ./build_cpu_nms.sh\n","%cd ..\n","\n","# !gdown --id 1SQsMhvAmpD1O8Hm0yEGom0C0rXtA0qs8\n","# !unzip -n 3dmm_data\n","!unzip -n /content/drive/MyDrive/Samal_work/3dmm_data.zip\n","\n","!gdown --id 1BVHbiLTfX6iTeJcNbh-jgHjWDoemfrzG\n","\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8m3_Bhz-Fvl","executionInfo":{"status":"ok","timestamp":1677620764392,"user_tz":-180,"elapsed":8815,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"ea48da52-3503-4043-8a81-0ec4e188eeec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/SynergyNet/SynergyNet/Sim3DR\n","running build_ext\n","skipping 'lib/rasterize.cpp' Cython extension (up-to-date)\n","/content/SynergyNet/SynergyNet/FaceBoxes\n","running build_ext\n","skipping 'nms/cpu_nms.c' Cython extension (up-to-date)\n","/content/SynergyNet/SynergyNet\n","Archive:  /content/drive/MyDrive/Samal_work/3dmm_data.zip\n","/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1BVHbiLTfX6iTeJcNbh-jgHjWDoemfrzG\n","To: /content/SynergyNet/SynergyNet/best.pth.tar\n","100% 75.4M/75.4M [00:00<00:00, 212MB/s]\n","\u001b[0m\u001b[01;34m3dmm_data\u001b[0m/              \u001b[01;34mimg\u001b[0m/                 singleImage.py\n","artistic.py             LICENSE              singleImage_simple.py\n","\u001b[01;34mbackbone_nets\u001b[0m/          \u001b[01;32mloss_definition.py\u001b[0m*  synergy3DMM.py\n","\u001b[01;32mbenchmark_aflw2000.py\u001b[0m*  main_train.py        synergy_demo.ipynb\n","\u001b[01;32mbenchmark.py\u001b[0m*           \u001b[01;32mmodel_building.py\u001b[0m*   train_script.sh\n","benchmark_validate.py   \u001b[01;34mpretrained\u001b[0m/          \u001b[01;34mutils\u001b[0m/\n","best.pth.tar            README.md            uv_texture_realFaces.py\n","\u001b[01;34mdemo\u001b[0m/                   setup.py\n","\u001b[01;34mFaceBoxes\u001b[0m/              \u001b[01;34mSim3DR\u001b[0m/\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","import numpy as np\n","import cv2\n","from utils.ddfa import ToTensor, Normalize\n","# from model_building import SynergyNet\n","from utils.inference import crop_img, predict_sparseVert, draw_landmarks, predict_denseVert, predict_pose, draw_axis\n","import argparse\n","import torch.backends.cudnn as cudnn\n","cudnn.benchmark = True\n","import os\n","import os.path as osp\n","import glob\n","from FaceBoxes import FaceBoxes\n","from utils.render import render\n","import scipy.io as sio"],"metadata":{"id":"grHxPa_0-Fs9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from model_building import SynergyNet"],"metadata":{"id":"EIDXk73p-Fqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('-f', '--files', default='./img/', help='path to a single image or path to a folder containing multiple images')\n","parser.add_argument(\"--png\", action=\"store_true\", help=\"if images are with .png extension\")\n","parser.add_argument('--img_size', default=120, type=int)\n","parser.add_argument('-b', '--batch-size', default=1, type=int)\n","\n","args = parser.parse_args()\n","# args.files = data_biwi_dict['rel_path'][:100]\n","args.files = '/content/faces_0/01/frame_00003_rgb.png'"],"metadata":{"id":"Y6mfeqJZ-FoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Following 3DDFA-V2, we also use 120x120 resolution\n","IMG_SIZE = 120\n","\n","# load pre-tained model\n","checkpoint_fp = '/content/drive/MyDrive/Samal_work/best_pose.pth.tar' \n","args.arch = 'mobilenet_v2'\n","args.devices_id = [0]\n","\n","checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n","    \n","model = SynergyNet(args)\n","model_dict = model.state_dict()\n","\n","# because the model is trained by multiple gpus, prefix 'module' should be removed\n","for k in checkpoint.keys():\n","    model_dict[k.replace('module.', '')] = checkpoint[k]\n","\n","model.load_state_dict(model_dict, strict=False)\n","# model = model.cuda()\n","model.eval()\n","\n","# face detector\n","face_boxes = FaceBoxes()\n","\n","# preparation\n","transform = transforms.Compose([ToTensor(), Normalize(mean=127.5, std=128)])"],"metadata":{"id":"cq_hCcrs-FlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _to_ctype(arr):\n","    if not arr.flags.c_contiguous:\n","        return arr.copy(order='C')\n","    return arr\n","tri = sio.loadmat('./3dmm_data/tri.mat')['tri'] - 1"],"metadata":{"id":"g4SKp6BHVyIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make CSV"],"metadata":{"id":"MtUrfRXc99kt"}},{"cell_type":"code","source":["class DatasetCSV():\n","  def __init__(self, path, args, checkpoint_fp='/content/drive/MyDrive/Samal_work/best_pose.pth.tar'):\n","    self.dataset = {}\n","    self.dataset['uid'] = None\n","    self.dataset['img_name'] = None\n","    self.dataset['rel_path'] = None\n","    self.dataset['resolution'] = None\n","    self.dataset['facerect'] = None\n","    self.dataset['landmarks'] = None\n","    self.dataset['pose_angles'] = None\n","    self.path = path\n","    self.args_model = args\n","    self.imgs_path = self.get_files(self.path) # default end_path='.png'\n","\n","    # self.dataset['img_name'] = self.get_img_name(self.imgs_path)\n","    self.dataset['rel_path'] = self.get_rel_path(self.imgs_path)\n","    # self.dataset['uid'] = self.get_uid(self.imgs_path)\n","    # self.dataset['resolution'] = self.get_resolution()\n","    self.dataset['facerect'], self.indexes = self.get_facerect() #indexes for facerects \n","    # self.dataset['landmarks'] = self.get_landmarks()\n","    # self.dataset['pose_angles'] = self.get_poses()\n","\n","  def get_files(self, path='./', end_path='/*.png', ext=('.info')):\n","    \"\"\" Get all image files \"\"\"\n","    path_sorted = sorted(os.listdir(path))\n","    files_all = []\n","    for file in path_sorted:\n","       files_path = os.path.join(path, file)\n","       files_list = glob.glob(f'{files_path}{end_path}')\n","       files_all.extend(files_list)\n","    return files_all\n","  def get_img_name(self, paths):\n","    res_img_name = []\n","    for path in paths:\n","      res_img_name.append(Path(path).stem)\n","    return res_img_name\n","  def get_rel_path(self, paths):\n","    res_rel_path = []\n","    for path in paths:\n","      # res_rel_path.append(path[9:]) #delete '/content/'\n","      res_rel_path.append(\"/\".join(path.strip(\"/\").split('/')[1:])) #delete first directory in path\n","    return res_rel_path\n","  def get_uid(self, paths):\n","    pathC = Counter(map(os.path.dirname, paths))\n","    res_uid = []\n","    for i, key in enumerate(pathC.keys()):\n","      res_uid.extend([i for n in range(pathC[key])])\n","    return res_uid\n","  def get_resolution(self):\n","    res_resolution = []\n","    for img_path in self.dataset['rel_path']:\n","      img = cv2.imread(img_path)\n","      h, w, d = img.shape\n","      res_resolution.append((h,w))\n","    return res_resolution\n","  def make_correct_rect(curr_rect): # x, y, w, h\n","    xmin, ymin, xmax, ymax, score = curr_rect\n","    w = xmax - xmin + 1\n","    h = ymax - ymin + 1\n","    return (xmin, ymin, w, h)\n","  def find_center_point(img_path):\n","    img_cent = cv2.imread(img_path)\n","    width, depth = np.shape(img_cent)[0], np.shape(img_cent)[1]\n","    x_center = width / 2\n","    y_center = depth / 2\n","    return (x_center, y_center)\n","  def make_list_of_rects(rects):\n","    x_cords = []\n","    y_cords = []\n","    for rect in rects:\n","      correct_rect = self.make_correct_rect(rect)\n","      x_cords.append(correct_rect[0])\n","      y_cords.append(correct_rect[1])\n","    A = np.column_stack((x_cords, y_cords))\n","    return A\n","  def get_facerect(self):\n","    files = self.dataset['rel_path']\n","    res_facerect = []\n","    indexes = {}\n","    for img_fp in tqdm.tqdm(files):\n","      img_ori = cv2.imread('/content/'+ str(img_fp))\n","      try:\n","        # crop faces\n","        rects = face_boxes(img_ori)\n","        if len(rects) == 1:\n","          res_facerect.append(self.make_correct_rect(rects[0]))\n","          indexes[img_fp] = 0\n","        else:\n","          center_of_img = self.find_center_point(img_fp)\n","          A = self.make_list_of_rects(rects)\n","          distances = np.linalg.norm(A-center_of_img, axis=1)\n","          min_index = np.argmin(distances)\n","          res_facerect.append(self.make_correct_rect(rects[min_index]))\n","          indexes[img_fp] = min_index\n","      except:\n","        res_facerect.append([None])\n","        indexes[img_fp] = None\n","    return res_facerect, indexes\n","def make_seven_landmarks(pts):\n","  y_cord, x_cord = [], []\n","  # nose, left_eye_left, left_eye_right, \n","  # right_eye_left, right_eye_right, \n","  # mouth_left, mouth_right\n","  # list_of_vert = [33, 36, 39, 42, 45, 48, 54]\n","  list_of_vert = [30, 36, 39, 42, 45, 48, 54]\n","  for ind in list_of_vert:\n","    x_cord.append(pts[0][ind])\n","    y_cord.append(pts[1][ind])\n","  finall_landms = tuple((x, y) for x, y in zip(x_cord, y_cord))\n","  return finall_landms\n","def get_landmarks(self):\n","  rects = self.dataset['facerect']\n","  res_landmarks = []\n","  for ind, img_fp in enumerate(self.indexes.keys()):\n","    img_ori = cv2.imread('/content/'+ str(img_fp))\n","    roi_box = rects[ind]\n","    if roi_box != [None]:\n","      # enlarge the bbox a little and do a square crop\n","      HCenter = (rect[1] + rect[3])/2\n","      WCenter = (rect[0] + rect[2])/2\n","      side_len = roi_box[3]-roi_box[1]\n","      margin = side_len * 1.2 // 2\n","      roi_box[0], roi_box[1], roi_box[2], roi_box[3] = WCenter-margin, HCenter-margin, WCenter+margin, HCenter+margin\n","      img = crop_img(img_ori, roi_box)\n","      img = cv2.resize(img, dsize=(IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n","      input = transform(img).unsqueeze(0)\n","      with torch.no_grad():\n","            # input = input.cuda()\n","            param = model.forward_test(input)\n","            param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n","\n","      # inferences\n","      lmks = predict_sparseVert(param, roi_box, transform=True)\n","      res_landmarks.append(self.make_seven_landmarks(lmks))\n","    else:\n","      res_landmarks.append([None])\n","  return res_landmarks\n","def make_pose_by_path(pose_path):\n","  pose_path = '/content/'+pose_path[:-8] + '_pose.txt'\n","  # print(pose_path)\n","  # Load pose in degrees\n","  pose_annot = open(pose_path, 'r')\n","  R = []\n","  for line in pose_annot:\n","   line = line.strip('\\n').split(' ')\n","   L = []\n","   if line[0] != '':\n","     for nb in line:\n","       if nb == '':\n","         continue\n","       L.append(float(nb))\n","     R.append(L)\n","  R = np.array(R)\n","  # print(R)\n","  T = R[3,:]\n","  R = R[:3,:]\n","  pose_annot.close()\n","  R = np.transpose(R)\n","\n","\n","  roll = -np.arctan2(R[1][0], R[0][0]) * 180 / np.pi\n","  yaw = -np.arctan2(-R[2][0], np.sqrt(R[2][1] ** 2 + R[2][2] ** 2)) * 180 / np.pi\n","  pitch = np.arctan2(R[2][1], R[2][2]) * 180 / np.pi\n","  # print(yaw, pitch, roll) # for plot it is correct\n","  return (roll, pitch, yaw)\n","def get_poses(self):\n","  files = self.dataset['rel_path']\n","  res_poses = []\n","  for img_fp in tqdm.tqdm(files):\n","    pose = make_pose_by_path(img_fp)\n","    res_poses.append(pose)\n","  return res_poses"],"metadata":{"id":"CbQjTuBDvOF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ABS_PATH = '/content/faces_0'\n","test_dataset_class = DatasetCSV(ABS_PATH, args)\n"],"metadata":{"id":"WrEZ-Q4u2xuv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677622795129,"user_tz":-180,"elapsed":954830,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"218c1a46-5732-462b-8aa7-21db7360628c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 15678/15678 [15:54<00:00, 16.43it/s]\n"]}]},{"cell_type":"code","source":["dataset_dict = test_dataset_class.dataset\n","print(dataset_dict.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kbhCpLN29MR","executionInfo":{"status":"ok","timestamp":1677622806231,"user_tz":-180,"elapsed":294,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"da2d0c36-81a7-4349-b3b9-4eae83ca8b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['uid', 'img_name', 'rel_path', 'resolution', 'facerect', 'landmarks', 'pose_angles'])\n"]}]},{"cell_type":"code","source":["print(len(dataset_dict['facerect']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zygS1a6l40ht","executionInfo":{"status":"ok","timestamp":1677622817017,"user_tz":-180,"elapsed":237,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"2a0b8c73-8acb-4722-a350-710f3e354a32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["15678\n"]}]},{"cell_type":"code","source":["print(dataset_dict['facerect'][1:10])"],"metadata":{"id":"7bASBjSa9fy5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677622869618,"user_tz":-180,"elapsed":344,"user":{"displayName":"bolatbek kubentayev","userId":"02292048037898381794"}},"outputId":"81ee6200-3171-426f-9699-00c54e45a393"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[None], [None], [None], [None], [None], [None], [None], [None], [None]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-rZU7EWoHSpw"},"execution_count":null,"outputs":[]}]}